# -*- coding: utf-8 -*-
"""Notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kmtGtKK9lTzt1PV4C3YvmdCzugcqNwYM
"""

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.utils import image_dataset_from_directory

import wget
from zipfile import ZipFile

## Preparing the images dataset
### Downloading and unzip the dataset

wget.download('https://github.com/carrionalfredo/Capstone_1/raw/main/dataset/Pistachio_image_Dataset.zip')

ZipFile('Pistachio_image_Dataset.zip', 'r').extractall(path='./images')

### Load and create train and evaluation images sets

batch_size = 32
img_height = 150
img_width = 150

train_ds = tf.keras.utils.image_dataset_from_directory(
  './images/',
  validation_split=0.2,
  seed = 1,
  subset="training",
  image_size=(img_height, img_width),
  batch_size=batch_size)

val_ds = tf.keras.utils.image_dataset_from_directory(
  './images/',
  validation_split=0.2,
  seed = 1,
  subset="validation",
  image_size=(img_height, img_width),
  batch_size=batch_size)

## Building the model

# import required packages from Keras
from keras.models import Sequential
from keras.layers import Rescaling, Dense, Dropout, Conv2D, MaxPool2D, Flatten, Activation

### Functions for building model and training the model

def model_build(name, droprate, learning_rate):
    model = Sequential(name = name)
    
    model.add(Rescaling(1./255))
    
    model.add(Conv2D(32,3,3, input_shape = (150,150,3), activation = 'relu'))
    
    model.add(MaxPool2D(2,2))
    
    model.add(Dropout(droprate, seed= 1))
    
    model.add(Flatten())
    
    model.add(Dense(32, activation='relu'))
    
    model.add(Dense(2, activation='softmax', name = 'output'))
    
    model.compile(
      optimizer = keras.optimizers.Adam(
          learning_rate = learning_rate
          ),
      loss=keras.losses.SparseCategoricalCrossentropy(),
          metrics=['accuracy']
          )
    return model

def train_model(name, droprate, learning_rate, batch_size, epochs):
  model = model_build(name, droprate, learning_rate)
  history = model.fit(
      train_ds,
      validation_data=val_ds,
      epochs=epochs,
      batch_size=batch_size,
      )
  return model, history.history

##Values for hyperparameters

droprate = 0.6
learning_rate=0.0001
epochs = 60

## Training the model

model = model_build(name='CS1_Model', droprate=droprate, learning_rate=learning_rate)

checkpoint = keras.callbacks.ModelCheckpoint(
    'CS1{epoch:02d}_{val_accuracy:.3f}.h5',
    save_best_only=True,
    monitor='val_accuracy',
    mode='max'
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=epochs,
    batch_size=batch_size,
    callbacks=[checkpoint]
    )

#model.save_weights('CS1_model.h5', save_format='h5')
